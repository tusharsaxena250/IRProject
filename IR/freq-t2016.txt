applic:2
selfdatasizeload:3
ani:2
tfplaceholdertffloat:7
selfactionload:3
permiss:2
express:2
except:2
import:7
see:2
initializeself:2
condit:2
govern:2
valuerl:6
selfworldmodel:3
softwar:2
obtain:2
limit:2
selfconfigmodelconfig:6
selfdoneload:3
unless:2
epoch:3
=:19
requir:2
deterministicworldmodel:2
either:2
varlist=valuerlpolicyparam:2
checkpointself:2
selfrewardload:3
sparamsindex:2
of:1
class:2
learner:3
policyoptim:2
qoptim:2
impli:2
policygv:3
pass:3
learnernameself:2
file:2
==============================================================================:2
train:2
copi:2
selfobsload:3
loadworldmodelself:2
selflearnerconfig:2
makecoremodelself:2
reserv:2
worldmodel:2
selfsavepath:4
return:6
kind:2
selfnextobsload:3
ospathexist:2
right:2
tf:2
law:2
deterministicworldmodelselfconfignam:2
selflearnerconfigbatchs:6
cappedqgv:2
thi:2
els:2
npprodselfenvconfigobsdim:3
warranti:2
selfloadworldmodel:3
option:2
or:1
selfcorecopytooldselfsess:2
qgv:3
write:2
specif:2
varlist=valuerlqparam:2
is:1
os:2
selfworldmodelloadselfsess:3
httpwwwapacheorglicenseslicens:2
basi:2
detail:2
policytrainop:3
qloss:3
cappedpolicygv:2
without:2
backupself:2
fals:6
tftrainadamoptimizer:3
apach:2
tensorflow:3
complianc:2
worldmodel=selfworldmodel:2
policyoptimizercomputegradientspolicyloss:2
numpi:2
policyloss:3
valuerlselfconfignam:2
inspectloss:3
qoptimizerapplygradientscappedqgv:2
selfenvconfig:3
may:3
selfenvconfigactiondim:2
function:3
agre:2
languag:2
loop:2
qoptimizercomputegradientsqloss:2
distribut:3
valuerlbuildtraininggraphselfcurrentbatch:2
true:2
%:4
selfbonuskwargsmodellock:3
valuerllearnerlearn:2
author:2
copyright:2
resumefromcheckpointself:2
policyoptimizerapplygradientscappedpolicygv:2
selfworldmodelsaveid:2
valuerlspecif:2
use:2
overrid:2
version:2
as:1
def:9
qtrainop:3
np:2
makeloaderplaceholdersself:2
none:2
licens:9
